# interview


# 笔试题1: spark处理kafka流式数据

Mysql里面有一个test表，如下:
host  uri  content  count  ctime
127.0.0.1  /test  host,uri 1  2019/01/07 11:00:23
192.168.1.1 /admin  username,pwd 1 2019/01/07 11:00:24

利用spark从kafka消费数据，判断是否和Mysql里面的数据host, uri, content是否重复。重复则更新计算器和时间，不重复则直接写入数据.


# 笔试题2: CSV 文件处理

给定一个 CSV 文件，其内容的展现规则如下：

- 每一行数据包含多个字段，字段间以[,]分割。
- 如果字段值不含有 [,] 和 ["] ，直接解析输出。
- 如果字段值内部含有逗号[,]，在在字段值两边加上双引号["]将字段值括起来。
- 如果字段值内部含有双引号["]，则字段值两边加上双引号["]括起来，同时，将字段值内的一个双引号["]替换为两个双引号 [""]，例如: [下棋,"飞"] 在 CSV 文件中被表现为 ["下棋,""飞"""]。

## 处理要求：

读入文件 cvs.txt，根据上述 csv 文件的规则进行解析，重新格式化字段生成输出文件 output.txt

将
第一列转为整形(int)
第二列为字符串型
第三列为字符串型
第四列转为浮点数（float）
第五列转为日期类型（DateTime）

输出文件的字段以制表符 [TAB] 来分割字段，
字符串字段输出时用单引号[']括起来
日期字段显示成 YYYY/MM/DD 的格式

说明：
1、可以假设字段值只包含单行数据，即字段值本身不含有 [回车换行]
2、不能对文件 csv.txt 作任何修改

## 编程要求:

使用任何你熟悉的编程语言编写，时间为 1.5 小时。
